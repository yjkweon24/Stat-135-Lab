---
title: "Stat 135 Lab 1 - Jin Kweon (3032235207)"
author: "Jin Kweon"
date: "2/20/2017"
output:
  pdf_document: default
  html_document: default
---

```{r include=FALSE}
# Use warn.conflicts just in case warning ups when running a package.
library(DataComputing, warn.conflicts =  FALSE) 
library(ggplot2, warn.conflicts =  FALSE)
library(MASS, warn.conflicts =  FALSE)
library(boot, warn.conflicts =  FALSE)
library(nleqslv, warn.conflicts =  FALSE)
library(stats4, warn.conflicts =  FALSE)
# Exclude warnings. 
options(warn=-1)
```

$I\ let\ "Data"\ as\ my\ first\ row\ for\ the\ data\\$
$I\ have\ to\ make\ the\ data\ be\ numeric,\ and\ let\ this\ be\ "newdata". \\$

#Data import
```{r}
whales <- read.csv("/whales.txt", header = FALSE, dec = ",", stringsAsFactors = FALSE)
colnames(whales) <- "Data"
options(digits=9, stringsAsFactors = FALSE)
whales$Data <- as.numeric(whales$Data)
newdata <- as.numeric(whales[[1]])
```

$I\ am\ drawing\ a\ histogram\ with\ whales.txt\ data.\\$ 
$After,\ I\ plotted\ gamma\ distribution\ with\ whales.txt\ to\ see\ it\ does\ fit.$

#A
```{r}
hist(newdata, main = "Whales Swimming", xlab = "Whales Time", ylab = "Frequency")
hist(rgamma(newdata, 0.9), main = "Whales Swimming with Gamma Fit",
     xlab = "Whales Time on Gamma", ylab = "Gamma fit frequency") 
# So, I can say the histogram looks like it fits into the gamma distribution. 
# The left portion is much bigger compared to the rightmost portions.
# The frequency gets smaller rapidly. -> fits into gamma. 
# To prove my statement, I used rgamma function and get the data, and plot them as histogram.
```

$\\$
$\\$

$I\ explained\ the\ formula\ and\ explanations\ in\ more\ detail\ on\ separate\ paper.$

#B
```{r}
# Method of Moments Way
# when I say x ~ gamma(alpha, lambda), and x_bar = mean of sample space (210),
# expectation(x^2) = get2, 
# alph(a) = (x_bar)^2 / [get2 - (x_bar)^2]  and
# lambd(a) = (x_bar) / [get2 - (x_bar)^2]
x<- function(){
x_bar = mean(newdata)
get2 = (x_bar)^2 + var(newdata)
alph_mom = (x_bar)^2 / (get2 - (x_bar)^2) # Get Alpha.
lambd_mom = (x_bar) / (get2 - (x_bar)^2) # Get Lambda
}

# unlist(replicate(100, expr = x, simplify = FALSE)) -> bootstrap

# Randomly get 100000 data from the alpha and lambda I got, to see how it fits into gamma distribution.
x <- rgamma(100000, shape=alph_mom, rate=lambd_mom) 
densit <- density(x)
datas <- data.frame(x = densit$x, y = densit$y)

# Plot density from Rgamma points with MoM
ggplot(data = datas, aes(x = x, y = y)) + 
  geom_point(size = 1) +
  theme_classic() + xlab("Whales Data on random gamma") + ylab("Density frequency") +
  ggtitle("Plot Density from random Gamma points with MoM")

cat("Alpha MoM is ", alph_mom, "\n")
cat("Lambda MoM is ", lambd_mom, "\n")
```

$Alpha\ from\ MoM\ is\ around\ 0.7953684843.\\$
$Lambda\ from\ MoM\ is\ around\ 1.3124913107$

$\\$

$I\ decided\ to\ go\ for\ using\ mle()\ function\ embedded\ in\ R\ coming\ along\ with\ stats4\ package.$

$\\$
$\\$

#C
```{r}
# Maximum Likehoold Way
# As the text book mentions "MLE are not given in closed form,
# obtaining their exact sampling distribution would appear to be intractable.
# We thus use the bootstrap to approximate these distributions." -> 
# So I went over "https://pdfs.semanticscholar.org/306d/d2f94d4e2a4e460ba26d07aba05c6f0b587a.pdf"
# and "https://en.wikipedia.org/wiki/Gamma_distribution".
# I found that Newton's Method is good and I came up with to alphas which are pretty acurate.
# lambda = alpha / x_bar
# 1. alpha ~ ((3-s + sqrt((s-3)^2 + 24*s)) / 12*s) where
# s = log(x_bar) - mean(log(newdata)) -> within 1.5% of the corrected value
# (https://en.wikipedia.org/wiki/Gamma_distribution) -> Turn out this is not really accurate ->
# lambda: 0.3117 and alpha: 0.1889
# 2. alpha ~ (0.5/s)->(https://pdfs.semanticscholar.org/306d/d2f94d4e2a4e460ba26d07aba05c6f0b587a.pdf)
# -> lambda: 2.39133 and alpha: 1.449144 -> This is pretty accurate.
# 3. alpha -> from the textbook -> nleqslv  
# 4. alpha -> from mle() function -> lambda: 2.63269 and alpha: 1.59541

# MLE function using package of stats4
MLEf <- function(alpha, lambda) {
     R = dgamma(whales$Data, alpha, lambda)
     -sum(log(R))
}

mle1 = mle(MLEf, start = list(alpha = 1, lambda = 1), method = "L-BFGS-B",
           lower = c(-Inf, 0), upper = c(Inf, Inf))
# Get alpha and lambda
alph_mle = mle1@coef[[1]]
lambd_mle = mle1@coef[[2]]

# Randomly get 50000 data from the alpha and lambda I got, to see how it fits into gamma distribution.
x2 <- rgamma(50000, shape=alph_mle, rate=lambd_mle)
densit2 <- density(x2)
datas2 <- data.frame(x_mle = densit2$x, y_mle = densit2$y)

# Plot density from Rgamma points with MLE
ggplot(data = datas2, aes(x = x_mle, y = y_mle)) + 
  geom_point(size = 1) +
  theme_classic() + xlab("Whales Data") + ylab("MLE Freq") +
  ggtitle("Plot Density from random Gamma points with MLE")

#Fit Parameters - Maximum likelihood fitting of univariate dist
fit.params2 <- fitdistr(x2, "gamma")

# Plot with density points - Fitparams into the MLE2 density
ggplot(data = datas2, aes(x = x_mle,y = y_mle)) + 
  geom_point(size = 2) +     
  geom_line(aes(x=datas2$x, y=dgamma(datas2$x, fit.params2$estimate["shape"],
                                     fit.params2$estimate["rate"])), color="blue", size = 0.5) + 
  theme_classic() + xlab("Whales Data") + ylab("MLE Freq") +
  ggtitle("Plot MLE fitting with a blue line")

cat("Alpha MLE is ", alph_mle, "\n")
cat("Lambda MLE is ", lambd_mle, "\n")
```

$Alpha\ from\ MLE\ is\ around\ 1.5954092877.\\$
$Lambda\ from\ MLE\ is\ around\ 2.63269285727$

$Alpha,\ lambda\ from\ MLE\ are\ approximately\ two\ factors\ of\ alpha,\ lambda\ from\ MoM,\ respectively.$
$\\$
$\\$

#D
```{r}
# rate = 1 / scale
hist1 <- ggplot(whales, aes(x = Data)) +
  geom_histogram(binwidth = 0.2, fill = "black", aes(y = ..density..),
                 position = "identity", size = 1) +
  xlab("whales time") +ylab("Freq height") + ggtitle("Whales Speed MLE (Red) vs MoM (Blue)")+
  stat_function(fun = dgamma, args= c(shape=alph_mle, rate=lambd_mle), color = "red")+
  stat_function(fun = dgamma, args= c(shape=alph_mom, rate=lambd_mom), color = "blue")
  
hist1 # Display GGplot
```

$Although,\ the\ curved\ lines\ are\ little\ bit\ below\ the\ histogram\, it\ looks\ pretty\ reasonable.$
$\\$
$\\$

#E
```{r}
#Make two groups for alpha and lambda to save 5000 bootstrapped values.
lambdhats <- numeric(0)
alphhats <- numeric(0)

#I bootstrapped 5000 times, to get mean and standard errors of MoM parameters. 
for (i in 1:5000){
  do <- rgamma(210, shape = alph_mom, rate = lambd_mom)
  
  xbar <- mean(do)
  s <- var(do)
  
  lambdhat <- (xbar / s)
  alphhat <- (xbar^2 / s)
  
  lambdhats <- c(lambdhats, lambdhat) #Save the new ones for each for loop.
  alphhats <- c(alphhats, alphhat)
}

cat("Mean for lambda hat - MoM", mean(lambdhats),"\n")
cat("sdv for lambda hat - MoM", sd(lambdhats),"\n")

cat("Mean for alpha hat - MoM", mean(alphhats),"\n")
cat("sdv for alpha hat - MoM", sd(alphhats),"\n")

hist(alphhats, main = "Bootstrap on alpha hat for MoM")
hist(lambdhats, main = "Bootstrap on lambda hat for MoM")
# Looks normal
```
$This\ is\ the\ bootstrap\ for\ MoM.\\$
$\\$
$Mean\ for\ \hat{\lambda} - MoM\ \approx\ 1.35\\$
$Standard\ Error\ for\ \hat{\lambda} - MoM\ \approx\ 0.21$
$\\$
$Mean\ for\ \hat{\alpha} - MoM\ \approx\ 0.81\\$
$Standard\ Error\ for\ \hat{\alpha} - MoM\ \approx\ 0.11\\$

$This\ is\ closed\ to\ normal\ and\ centralized\ in\ the\ middle,\ and\ standard\ errors\ for\ alpha\ and\ lambda$
$\ are\ small\ so,\ I\ can\ say\ they\ are\ pretty\ good\ approximates.$
$\\$
$\\$

#F
```{r}
#Make two groups for alpha and lambda to save 5000 bootstrapped values.
lambdhats2 <- numeric(0)
alphhats2 <- numeric(0)

for(i in 1:5000){
  do2 <- rgamma(210, shape = alph_mle, rate = lambd_mle)
  
  # I made new function with alpha and lambda, and use mle() function to bootstrap.
  LL2 <- function(alpha, lambda) {
     R = dgamma(do2, alpha, lambda)
     -sum(log(R))
  }
  mle2 = mle(LL2, start = list(alpha = alph_mle, lambda=lambd_mle), method =
               "L-BFGS-B", lower = c(-Inf, 0), upper = c(Inf, Inf))
  
  lambdhat2 <- mle2@coef[[2]]
  alphhat2 <- mle2@coef[[1]]
  lambdhats2 <- c(lambdhats2, lambdhat2) #Save the new ones for each for loop.
  alphhats2 <- c(alphhats2, alphhat2)
}

cat("Mean for lambda hat - MLE", mean(lambdhats2), "\n")
cat("sdv for lambda hat - MLE", sd(lambdhats2),"\n")

cat("Mean for alpha hat - MLE", mean(alphhats2),"\n")
cat("sdv for alpha hat - MLE", sd(alphhats2),"\n")

hist(alphhats2, main = "Bootstrap on alpha hat for MLE")
hist(lambdhats2, main = "Bootstrap on lambda hat for MLE")
# Looks normal
```

$This\ is\ the\ bootstrap\ for\ MLE.\\$

$Mean\ for\ \hat{\lambda} - MLE\ \approx\ 2.67\\$
$Standard\ Error\ for\ \hat{\lambda} - MLE\ \approx\ 0.28$
$\\$
$Mean\ for\ \hat{\alpha} - MLE\ \approx\ 1.61\\$
$Standard\ Error\ for\ \hat{\alpha} - MLE\ \approx\ 0.14\\$

$This\ is\ closed\ to\ normal\ and\ centralized\ in\ the\ middle,\ and\ standard\ errors\ for\ alpha\ and\ lambda$
$\ are\ small\ so,\ I\ can\ say\ they\ are\ pretty\ good\ approximates.$
$\\$
$\\$

#G
```{r}
# Three methods for confidence interval for MLE: exact method,
# approximations based on the large sample properties of MLE,
# and bootstrap confidence intervals. -> I go for bootstrap one. (pg. 283)
# Since Bootstrap uses a lot of data, by the central theorem,
# we say the distribution is likely to be a normal.(But not always.)

# Lambda

# For 90% - 4500 points in the interval (250 points in each side)
bottom250 <- head(sort(lambdhats2),250) #Sort in order, Get small 250
low250_90 <- bottom250[250] #250th largest
top250 <- tail(sort(lambdhats2),250) #Sort in order, Get large 250
top250_90 <- top250[1] #4750th largest
hat_delta_90 <- top250_90 - lambd_mle #Get delta hat
low_bound_90 <- lambd_mle - hat_delta_90 #Get low boundary for confidence interval
low_delta_90 <- low250_90 - lambd_mle #Get delta underscore
high_bound_90 <- lambd_mle - low_delta_90 # Get high boundary for confidence interval

# For 95% - 4750 points in the interval (125 points in each side)
bottom125 <- head(sort(lambdhats2),125) #Sort in order, Get small 125
low125_95 <- bottom125[125]
top125 <- tail(sort(lambdhats2),125) #Sort in order, Get large 125
top125_95 <- top125[1]
hat_delta_95 <- top125_95 - lambd_mle
low_bound_95 <- lambd_mle - hat_delta_95
low_delta_95 <- low125_95 - lambd_mle
high_bound_95 <- lambd_mle - low_delta_95

# For 99% - 4950 points in the interval (25 points in each side)
bottom25 <- head(sort(lambdhats2),25) #Sort in order, Get small 25
low25_99 <- bottom25[25]
top25 <- tail(sort(lambdhats2),25) #Sort in order, Get large 25
top25_99 <- top25[1]
hat_delta_99 <- top25_99 - lambd_mle
low_bound_99 <- lambd_mle - hat_delta_99
low_delta_99 <- low25_99 - lambd_mle
high_bound_99 <- lambd_mle - low_delta_99



# Alpha

# For 90% - 4500 points in the interval
bottom250b <- head(sort(alphhats2),250)
low250_90b <- bottom250b[250]
top250b <- tail(sort(alphhats2),250)
top250_90b <- top250b[1]
hat_delta_90b <- top250_90b - alph_mle
low_bound_90b <- alph_mle - hat_delta_90b
low_delta_90b <- low250_90b - alph_mle
high_bound_90b <- alph_mle - low_delta_90b

# For 95% - 4750 points in the interval
bottom125b <- head(sort(alphhats2),125)
low125_95b <- bottom125b[125]
top125b <- tail(sort(alphhats2),125)
top125_95b <- top125b[1]
hat_delta_95b <- top125_95b - alph_mle
low_bound_95b <- alph_mle - hat_delta_95b
low_delta_95b <- low125_95b - alph_mle
high_bound_95b <- alph_mle - low_delta_95b

# For 99% - 4950 points in the interval
bottom25b <- head(sort(alphhats2),25)
low25_99b <- bottom25b[25]
top25b <- tail(sort(alphhats2),25)
top25_99b <- top25b[1]
hat_delta_99b <- top25_99b - alph_mle
low_bound_99b <- alph_mle - hat_delta_99b
low_delta_99b <- low25_99b - alph_mle
high_bound_99b <- alph_mle - low_delta_99b



print("Lambda")
print("Lower Boundary for 90%") 
print(low_bound_90)
print("Upper Boundary for 90%")
print(high_bound_90)
print("Lower Boundary for 95%")
print(low_bound_95)
print("Upper Boundary for 95%")
print(high_bound_95)
print("Lower Boundary for 99%") 
print(low_bound_99)
print("Upper Boundary for 99%")
print(high_bound_99)
print("=====================================")
print("=====================================")
print("=====================================")
print("Alpha")
print("Lower Boundary for 90%") 
print(low_bound_90b)
print("Upper Boundary for 90%")
print(high_bound_90b)
print("Lower Boundary for 95%")
print(low_bound_95b)
print("Upper Boundary for 95%")
print(high_bound_95b)
print("Lower Boundary for 99%") 
print(low_bound_99b)
print("Upper Boundary for 99%")
print(high_bound_99b)
```
$\\$
$(\hat{\lambda} - \overline{\delta}, \hat{\lambda} - \underline{\delta})\ for\ 90\% \approx\ (2.081,\ 3.018) \\$
$(\hat{\lambda} - \overline{\delta}, \hat{\lambda} - \underline{\delta})\ for\ 95\% \approx\ (1.969,\ 3.089) \\$
$(\hat{\lambda} - \overline{\delta}, \hat{\lambda} - \underline{\delta})\ for\ 99\% \approx\ (1.728,\ 3.228) \\$
$\\$
$(\hat{\alpha} - \overline{\delta}, \hat{\alpha} - \underline{\delta})\ for\ 90\% \approx\ (1.312,\ 1.795) \\$
$(\hat{\alpha} - \overline{\delta}, \hat{\alpha} - \underline{\delta})\ for\ 95\% \approx\ (1.259,\ 1.829) \\$
$(\hat{\alpha} - \overline{\delta}, \hat{\alpha} - \underline{\delta})\ for\ 99\% \approx\ (1.134,\ 1.903) \\$

